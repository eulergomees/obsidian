## Regra de Hebb
- Ajuste excitat√≥rio: se a sa√≠da produzida pela rede est√° coindidente com a sa√≠da esperada, os pesos sin√°pticos ser√£o est√£o incrementados proporcionalmente aos valores dos sinais de entrada.
- Ajuste inibit√≥rio: Se a sa√≠da produzida pela rede √© diferente da sa√≠da esperada, os pesos sin√°pticos ser√£o ent√£o decrementados proporcionalmente aos valores dos sinais de entrada.
$$
\bar{\omega i} = \bar{\omega} i-1 + \eta \cdot (y - \dot{y}) \cdot \bar{xi}
$$
- wi-1 = O vetor de pesos da √©poca anterior (indicada por i-1);
- eta = A taxa de aprendizagem que determina a velocidade e qualidade da converg√™ncia do erro;
- (y - y') = ùö´, erro, toler√¢ncia: a diferen√ßa entre o valor esperado y e o valor previsto y';
- xi = O vetor com os valores de entrada na √©poca atual i.

**Curva de converg√™ncia de erro:** mecanismo para verificar se a rede est√° realmente otimizando o problema.
## Fases de treinamento

Fase *forward*:
$$
u = \sum_{i=1} Wi \cdot Xi - \theta
$$
$$
y = g(u)
$$
Fase *backward*:
$$
\bar{\omega i} = \bar{\omega} i-1 + \eta \cdot (y - \dot{y}) \cdot \bar{xi}
$$
